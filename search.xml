<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Reinforcement Learning Introduction</title>
      <link href="/2022/02/12/Reinforcement-Learning-Introduction/"/>
      <url>/2022/02/12/Reinforcement-Learning-Introduction/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="introduction">Introduction</h1><p>After submitting a paper to ICML2022 in Jan, I feel really unconfident in my English academic writing since my supervisor almost polished every sentences in the paper for me and iterated the paper until 3 am, the deadline was three hours away. Although my boss is a easy-going person, he suggested me to improve my writing and presentation skills which are important for my Ph.d defense and job interviews. Therefore, I realize that I need to write something in English, not restricted to my mother tongue.</p><p>This semester, CS department is giving a course 'Reinforcement Learning' which I am very interested in. However, I am having an full-time internship so I can't enroll in this class. Hence, I decided to read the textbook on my own. I will write down the key contents of each chapter in the following days.</p><h2 id="text-book">text book</h2><p><a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement Learning: An Introduction</a><br /><a href="http://incompleteideas.net/">Richard S. Sutton</a> and <a href="https://people.cs.umass.edu/~barto/">Andrew G. Barto</a>.</p><h2 id="elements-of-reinforcement-learning">Elements of Reinforcement Learning</h2><p>A reinforcement learning system consists of : a <em>policy</em>, a <em>reward</em>, a <em>value function</em> and a <em>model</em> of environment.</p><h3 id="policy">Policy</h3><p>Policy defines the way of an agent behaving at a specific environment(timestep, states of the system, action set). It corresponds to a set of <em>stimulus-reponse</em> map. Assuming all agents are rational, the policy of each agent will correspond to the action bring maximum reward in term of value function at the state at given timestep in the environment.</p><h3 id="reward-and-value">Reward and Value</h3><p>Reward defines the goal of a reinforcement learning problem. After agents make action at each step, the environment will convert to next state according to the action made by agents while agents will receive rewards in an immedidate sense. The goal of each agent is to maximize the total rewards from the beginning to the end(summation of rewards at all rounds). However, value of state specifies what is good in the long run. It is not so straightforward to tell how much value a state has, not as simple as reward does. For example, a state might have a very low reward for each agent in the system but this state might bring very high rewards in the following round. Then we can say this state has high value.</p><h3 id="model">Model</h3><p>Model needs to define how the environment changes according to the actions made by agents. The common way to describe the environment is using <strong>transition matrix</strong>. If we can explicitly compute the transition matrix to plan, we can call this <strong>model-based</strong> methods. However, it is strict to demand transition matrix. Most of the time, we can not know the transition matrix. Apart from model-based methods, <strong>model-free</strong> methods which simultaneously learns the model of environment by trial and error are more practical for reinforcement learning problems.</p>]]></content>
      
      
      <categories>
          
          <category> Reading Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning Chap 2</title>
      <link href="/2022/02/12/Reinforcement-Learning-Chap-2/"/>
      <url>/2022/02/12/Reinforcement-Learning-Chap-2/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 id="multi-armed-bandits">Multi-armed Bandits</h1><p>Bandit? <img src="bandit.jpg" alt="avatar" /></p><p>Nope！ <strong>Multi-armed Bandits problem</strong> is a very basic problem in <a href="https://tor-lattimore.com/downloads/book/book.pdf">online learning</a>(not meaning internet education) which has overlapping part with reinforcement learning. Different from reinforcement learning, there is a strict and explicit mathematic framework to analyze the bound of regret and convergence rate of Multi-armed Bandits problem. Probability theory, Stochastic process and Markov chain are the tools to analyze bandits algorithms.</p><h2 id="a-k-armed-bandit-problem">A k-armed Bandit Problem</h2><p>You are facing repeatly with a choice among <span class="math inline">\(k\)</span> different options and you will get corresponding reward after you make a choice. If all rewards of options are deterministic, the player must choose the option with highest value all the time. Even in a stochastic bandit problem, suppose we know the means and variances of rewards, the player still always choose the action with highest expected reward: <span class="math display">\[q_{*}(a) \doteq \mathbb{E}\left[R_{t} \mid A_{t}=a\right]\]</span> However, if we assume that you do not know the action values with certainty, although you may have estimates. We would like to use <span class="math inline">\(Q_{t}(a)\)</span> to be an estimation of <span class="math inline">\(q_{*}(a)\)</span> at time step <span class="math inline">\(t\)</span>. <img src="Q.png" alt="avatar" /> Since <span class="math inline">\(Q_{t}(a)\)</span> is just an estimation, we might converge in a local optimum because of scarcity of knowledge of other actions if we take greedy policy. Apart from <em>exploiting</em> the information we have already, we also need <em>exploring</em> to attempt other potential good actions. Here is the idea of <span class="math inline">\(\epsilon\)</span>-greedy algorithm. You can see some results with or without <em>exploring</em>. <img src="e-greedy.png" alt="avatar" /></p><h2 id="incremental-implementation">Incremental Implementation</h2><p>From the previous section, we know the definition of <span class="math inline">\(Q_{t}(a)\)</span>, now we simplify it as follow, <span class="math display">\[Q_{n} \doteq \frac{R_{1}+R_{2}+\cdots+R_{n-1}}{n-1}\]</span> where <span class="math inline">\(n-1\)</span> is the number of one action being chosen. However, this is not a good way to update <span class="math inline">\(Q_{n}\)</span> since we have to summate all the rewards again and again when we are updating. A smaller way to update <span class="math inline">\(Q_{t}(a)\)</span> is to utilize the relationship between <span class="math inline">\(Q_{n+1}\)</span> and <span class="math inline">\(Q_{n}\)</span>. <span class="math display">\[\begin{aligned}Q_{n+1} &amp;=\frac{1}{n} \sum_{i=1}^{n} R_{i} \\&amp;=\frac{1}{n}\left(R_{n}+\sum_{i=1}^{n-1} R_{i}\right) \\&amp;=\frac{1}{n}\left(R_{n}+(n-1) \frac{1}{n-1} \sum_{i=1}^{n-1} R_{i}\right) \\&amp;=\frac{1}{n}\left(R_{n}+(n-1) Q_{n}\right) \\&amp;=\frac{1}{n}\left(R_{n}+n Q_{n}-Q_{n}\right) \\&amp;=Q_{n}+\frac{1}{n}\left[R_{n}-Q_{n}\right]\end{aligned}\]</span></p><p>The above eqution is something we are very familiar with in Gradient Descent.<span class="math inline">\(\frac{1}{n}\)</span> can be seen as a step size, and <span class="math inline">\(R_{n} - Q_{n}\)</span> is the gradient. For example, <img src="update.png" alt="avatar" /> <img src="bandit.png" alt="avatar" /></p><h2 id="upper-confidence-bound-action-selection">Upper-Confidence-Bound Action Selection</h2><p><span class="math inline">\(\epsilon\)</span>-greedy policy forces the player to try non-greedy actions but without any preference(just uniformly pick) but we wonder if there is a better way to select among the non-greedy actions according to their potential for actually being optimal? Upper-Confidence-Bound(UCB) is a very classic and popular（with many many variants）algorithm in bandits problem. The difference between greedy and UCB algorithm can be seen as follow: <span class="math display">\[\text{greedy: } A_{t} \doteq \underset{a}{\arg \max }\left[Q_{t}(a)\right]\]</span> <span class="math display">\[\text{UCB: } A_{t} \doteq \underset{a}{\arg \max }\left[Q_{t}(a)+c \sqrt{\frac{\ln t}{N_{t}(a)}}\right]\]</span> We can see in the above action policies, we have the second term which is called upper confidence bound. <span class="math inline">\(c\)</span> is a hyper-parameter, <span class="math inline">\(t\)</span> is time step and <span class="math inline">\(N_{t}(a)\)</span> is the number of choosing action <span class="math inline">\(a\)</span>. <span class="math inline">\(c &gt; 0\)</span> can be seen a degree of exploration. The more action <span class="math inline">\(a\)</span> is chosen, the less UCB of this action <span class="math inline">\(a\)</span> is so <span class="math inline">\(a\)</span> has less chance to be chosen again. At the same time, <span class="math inline">\(t\)</span> increases but <span class="math inline">\(N_t(a)\)</span> does not; because <span class="math inline">\(t\)</span> appears in the numerator, the uncertainty estimate increases. Eventually, actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time. <img src="UCB.png" alt="avatar" /></p><h2 id="gradient-bandit-algorithms">Gradient Bandit Algorithms</h2><p>We mentioned Gradient Descent before, which is the foundation of machine learning(we all learned GD when we were having a ML course). In this section, we consider a numerical <em>preference</em> for each action <span class="math inline">\(a\)</span>, <span class="math display">\[\operatorname{Pr}\left\{A_{t}=a\right\} \doteq H_{t}(a) = \frac{e^{H_{t}(a)}}{\sum_{b=1}^{k} e^{H_{t}(b)}} \doteq \pi_{t}(a)\]</span> We hope to update <span class="math inline">\(H_{t}(a)\)</span> in a Gradient-Descent way. For example, <span class="math display">\[H_{t+1}(a) \doteq H_{t}(a)+\alpha \frac{\partial \mathbb{E}\left[R_{t}\right]}{\partial H_{t}(a)}\]</span> where the measure of performance here is the expected reward: <span class="math display">\[\mathbb{E}\left[R_{t}\right]=\sum_{x} \pi_{t}(x) q_{*}(x)\]</span> We call it Bandit Gradient Algorithm. The derivation of the gradient is quite complited but intesting: <span class="math display">\[\begin{aligned}\frac{\partial \mathbb{E}\left[R_{t}\right]}{\partial H_{t}(a)} &amp;=\frac{\partial}{\partial H_{t}(a)}\left[\sum_{x} \pi_{t}(x) q_{*}(x)\right] \\&amp;=\sum_{x} q_{*}(x) \frac{\partial \pi_{t}(x)}{\partial H_{t}(a)} \\&amp;=\sum_{x}\left(q_{*}(x)-B_{t}\right) \frac{\partial \pi_{t}(x)}{\partial H_{t}(a)}\end{aligned}\]</span> Where <span class="math inline">\(B_t\)</span>, call the <em>baseline</em>, can be any scalar that does not depend on <span class="math inline">\(x\)</span>. The last eqution holds because <span class="math display">\[\sum_{x}\frac{\partial \pi_{t}(x)}{\partial H_{t}(a)} = 0\]</span> Let's continue, <span class="math display">\[\frac{\partial \mathbb{E}\left[R_{t}\right]}{\partial H_{t}(a)}=\sum_{x} \pi_{t}(x)\left(q_{*}(x)-B_{t}\right) \frac{\partial \pi_{t}(x)}{\partial H_{t}(a)} / \pi_{t}(x)\]</span> Then we can get the form of an expectation, <span class="math display">\[\begin{aligned}\frac{\partial \mathbb{E}\left[R_{t}\right]}{\partial H_{t}(a)} &amp;=\mathbb{E}\left[\left(q_{*}\left(A_{t}\right)-B_{t}\right) \frac{\partial \pi_{t}\left(A_{t}\right)}{\partial H_{t}(a)} / \pi_{t}\left(A_{t}\right)\right] \\&amp;=\mathbb{E}\left[\left(R_{t}-\bar{R}_{t}\right) \frac{\partial \pi_{t}\left(A_{t}\right)}{\partial H_{t}(a)} / \pi_{t}\left(A_{t}\right)\right]\end{aligned}\]</span> where here we have chosen the baseline <span class="math inline">\(B_{t}=\bar{R}_{t}\)</span>. Then we consider <span class="math inline">\(\frac{\partial \pi_{t}(x)}{\partial H_{t}(a)}\)</span>, utilizing standard quotient rule for derivatives: <span class="math display">\[\begin{aligned}\frac{\partial \pi_{t}(x)}{\partial H_{t}(a)} &amp;=\frac{\partial}{\partial H_{t}(a)} \pi_{t}(x) \\&amp;=\frac{\partial}{\partial H_{t}(a)}\left[\frac{e^{H_{t}(x)}}{\sum_{y=1}^{k} e^{H_{t}(y)}}\right] \\&amp;=\frac{\frac{\partial e^{H_{t}(x)}}{\partial H_{t}(a)} \sum_{y=1}^{k} e^{H_{t}(y)}-e^{H_{t}(x)} \frac{\partial \sum_{y=1}^{k} e^{H_{t}(y)}}{\partial H_{t}(a)}}{\left(\sum_{y=1}^{k} e^{H_{t}(y)}\right)^{2}} \\&amp;=\frac{\mathbb{1}_{a=x} e^{H_{t}(x)} \sum_{y=1}^{k} e^{H_{t}(y)}-e^{H_{t}(x)} e^{H_{t}(a)}}{\left(\sum_{y=1}^{k} e^{H_{t}(y)}\right)^{2}} \\&amp;=\frac{\mathbb{1}_{a=x} e^{H_{t}(x)}}{\sum_{y=1}^{k} e^{H_{t}(y)}-\frac{e^{H_{t}(x)} e^{H_{t}(a)}}{\left(\sum_{y=1}^{k} e^{H_{t}(y)}\right)^{2}}} \\&amp;=\mathbb{1}_{a=x} \pi_{t}(x)-\pi_{t}(x) \pi_{t}(a) \\&amp;=\pi_{t}(x)\left(\mathbb{1}_{a=x}-\pi_{t}(a)\right)\end{aligned}\]</span> Finally, we get <span class="math display">\[H_{t+1}(a)=H_{t}(a)+\alpha\left(R_{t}-\bar{R}_{t}\right)\left(\mathbb{1}_{a=A_{t}}-\pi_{t}(a)\right), \quad \text { for all } a\]</span> Baseline <span class="math inline">\(B_t\)</span> can shrink the gradient and make sure the preference <span class="math inline">\(H_{t}(a)\)</span> will not change too soon. The experiment results show Gradient Bandit Algorithms with a baseline outperform without a baseline. <img src="baseline.png" alt="avatar" /></p>]]></content>
      
      
      <categories>
          
          <category> Reading Note </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>失眠是枕头之上无尽的流浪</title>
      <link href="/2018/06/08/%E5%A4%B1%E7%9C%A0%E6%98%AF%E6%9E%95%E5%A4%B4%E4%B9%8B%E4%B8%8A%E6%97%A0%E5%B0%BD%E7%9A%84%E6%B5%81%E6%B5%AA/"/>
      <url>/2018/06/08/%E5%A4%B1%E7%9C%A0%E6%98%AF%E6%9E%95%E5%A4%B4%E4%B9%8B%E4%B8%8A%E6%97%A0%E5%B0%BD%E7%9A%84%E6%B5%81%E6%B5%AA/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>2014年6月8日的凌晨三点，我在菊园403还没有睡着，看着带有荧光指针的闹钟滴答滴答地走着，心里的情绪十分混沌。早上的语文考得很不理想。高中有一个很不好的习惯，喜欢考完试就对答案，月考期末考喜欢对选择题，对大题，我和同样喜欢对答案的吕狗经常在路上谈论数列的极限，带电粒子的旋转轨迹，然后或因全对（两个人答案都错的几率很小）而喜悦，或因伤亡惨重而沉默。然而，对答案并不是因为自信，恰恰是因为内心的不自信，就像一个得重病的患者更希望主动询问病情，而不是医生在他自以为健康的时候给予一记重锤。高考的语文过后，我并没有对答案。当我对答案之前，就感觉我考的不好的时候，那就真的是考得超级不好。不知道是不是受了影响，下午的数学不难，只是一道压轴题没有做出来，寄希望于数学上145的我盯着考场的闹钟，分分秒秒在走，自己却无能为力的绝望，只希望时间快点过去。晚上回到教室自习，氛围和前一晚明显有了变化。前一晚，大家的心随着炎热的天气而变得浮躁，同桌之间开始窃窃私语，讨论高考之后去海边的旅行，讨论世界杯的来临，或多或少来掩饰自己的紧张。那一晚，教室似乎陷入了冰点，大家默默看着自己的书，也不想在讨论什么。回到宿舍，舍友很早就关灯睡觉了，不知道为什么我就睡不着。已经三点了，第二天明明是一辈子最重要的考试了，自己的心却还不放过自己，想着为什么那道题自己做不出来，想着自己考不上北大会不会复读，想着如果时光能够倒流一天，想着高中三年的往事...不知不觉已经六点半了，我想我应该睡着了一会，但是不敢再趟因为怕睡晚了。我起得比舍友都早，早早出了门，想着以这种精神如何去面对理综。没有吃早餐，买了一瓶柠檬茶，一盒“午夜风暴”薄荷糖。第二天的佛系考试，以弃疗的心态考完了理综，大家在吐槽理综的时候，我只是默默的不吭声。下午的英语，我的心态出奇平和，仿佛在做一场月考。结果，理综考得最好，英语正常发挥。至今觉得，一切似乎都是命运的安排。</p><p>2018年6月8日，不知为何又失眠到三点。没有想着四年前的事，可能是因为一瓶阿萨姆，可能是担心两天后的托福考试，可能是再次考虑自己的命运。有些事，我想了四年，竟然也没想明白。原来自己一路跌跌撞撞，想的事情多数都是一些所谓的沉没成本，并不是问题的本质，想了很多其实什么都没想，很多的选择都是意气之举。不过，时间真的可以治疗一切。尽管如此，却不能治愈，消除心中遗憾唯一的方法就是弥补这个遗憾。</p><p>我，无论如何，也要做到。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回忆 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>疯狂钻石</title>
      <link href="/2018/06/05/%E7%96%AF%E7%8B%82%E9%92%BB%E7%9F%B3/"/>
      <url>/2018/06/05/%E7%96%AF%E7%8B%82%E9%92%BB%E7%9F%B3/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>忙碌奔波了大半年，终于在今天被击倒了。感觉很累，精神与身体都很疲惫。听着Pink Folyd的《Shine on you crazy diamond》竟然在教室睡着了。不得不说，这首歌无比深邃，让人想起很多事，想起《银翼杀手》的潮湿雨夜，想起Syd Barrett的堕落，感到人生无穷无尽的失落。Syd Barrett本是Pink Folyd的灵魂人物，光芒万丈，众星捧月。Syd本人用他的超脱世俗的灵感让Pink Folyd成名于世界，最后也因为他的灵感毁灭了他。也许，天才如同恒星般闪耀，也如同恒星般燃烧殆尽直到生命终结，永不妥协。</p><p>感觉命运就是一直循环，同样的泥潭，人总会一次又一次踏入。We're just two lost souls swimming in a fish bowl, year after year.</p>        <div id="aplayer-swLMgtYs" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;width: 100%;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-swLMgtYs"),            narrow: false,            autoplay: true,            showlrc: false,            music: {              title: "Shine on you crazy diamond",              author: "Pink Floyd",              url: "/bgm/Shine on You Crazy Diamond.mp3",              pic: "/cover.jpg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pink Floyd </tag>
            
            <tag> music </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>除夕</title>
      <link href="/2018/02/16/%E9%99%A4%E5%A4%95/"/>
      <url>/2018/02/16/%E9%99%A4%E5%A4%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>每年的除夕都会回乡下祭拜神灵，而每次去祭拜地藏王菩萨的时候，庙里的看护人总是会给我一个红包。那是一个年老的长者，五六年前家里遭遇了不幸，妻子离世儿子疯掉，想必是上辈子造孽，于是想着服侍菩萨换取老天的宽恕。农村人的哲学思想非常朴素，相信因果报应。他知道我的名字，而我却只依稀得记得这个人的脸，却不知道怎么称呼，什么辈分，只能微笑点头回应。小时候曾经在乡下住了差不多十年，那些记忆已经很模糊了。一路上，爸跟许多人打招呼，有些是他儿时玩伴，有些是他小学同学，还有是以前的邻居。在农村，很多人是一辈子都在这里了，靠着古老的族亲礼仪相处了几十年。</p><p>在宗族的老祠堂，供奉着附近几个村落的共同的先祖，祠堂也是由一位老汉看护，他看护了几十年了，没有娶妻，没有后代，靠着村委给的钱和种马铃薯过过日子。他和前来祭拜的每一个人都会聊几句，无外乎是谁谁在外做生意赚了钱，谁谁生了几个儿子。不知为何，听着乡里人谈钱一点儿也不觉得市侩，俗气，他们是朴素的农民，朴素得眼中只有生活的本质，生存和繁衍。也许有人觉得俗气，但我却觉得他们无比纯粹，城里人有时候绕了一大圈无非也是回到原点，不过显得比较精致罢了。农村的社会关系，其实也不过是社会的简化版本，更加直接，更加朴素，更加赤裸，生老病死，生存繁衍，利益纷争，“吃人”的礼仪教化。</p><p>村里的年轻人都出去了，长者们依旧主持着千百年来的那一套。看着日渐冷清的乡村，我知道这一切很快都不复存了。想起孩童时候，跟着爷爷奶奶住，受了周围邻居长辈不少照顾，对于这村子确是有一种说不出的情感。也许是阶级感情，亦或者是怀旧的情感。许多面孔已经不在了，老人离世，就连儿时的玩伴也有离开人世的。这世间的种种，芸芸众生无法参透其玄机，只能用神灵来自我安慰，神灵本就是芸芸众生自我安慰的美好幻想，企图去解释一种未知的东西，命运这东西啊。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>魔术与魔法</title>
      <link href="/2018/02/08/%E9%AD%94%E6%9C%AF%E4%B8%8E%E9%AD%94%E6%B3%95/"/>
      <url>/2018/02/08/%E9%AD%94%E6%9C%AF%E4%B8%8E%E9%AD%94%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>前几天又看了《致命魔术》一遍。</p><p>电影中，Borden和Angier两个命运交织在一起的魔术师，为着各自心中完美的魔术而做出了许多牺牲，影片本身的精彩无需赘述，但两人的魔术却引发了我的思考。Borden的魔术是依靠外人就连妻子都不得而知的孪生兄弟作为替身实现“移形换影”，是魔术表演技巧；Angier却是依靠特斯拉制造的科幻机器复制自己，实质上已经可以叫做魔法了（普通人无法做到无法解释）。</p><p>“魔术”与“魔法”，前段时间AI三巨头之一的Yann LeCun就对市面上一些所谓的智能机器人进行怒斥，他这样形容，“它相比于真正的AI，就像变戏法相对于真正的魔法，我们应该称它为草包AI”。这些草包AI所有对外的响应不过是人提前在编程的时候加入种种条件判断以及调用函数作出响应，并非是AI经过思考作出的响应。这样的行为无非是变戏法，变魔术，大家也知道魔术师做了手脚，大家并非真的追究人是否真的瞬间转移还是在空中漂浮，只是去围观那么一种视觉效果罢了。</p><p>然而，创造出真正的AI就如同和使用魔法一样遥不可及。</p><p>上世纪60年代，AI的概念刚刚提出的时候，很多科学家耗尽了许多心血与才华，从控制论的角度想要仿制出有着和人类大脑相同结构的电子脑，毕竟在不知道根本机理的情况下，模仿是最容易实现的一种方式。但是，人体是及其精密而复杂的系统，当时的技术水平根本不可能成功，这种仿制的思想陷入了瓶颈。这时人们思考着，汽车能跑并非是需要和马一样的结构，飞机也并非和鸟类相同的飞行原理，鸟类通过扇动翅膀使气往后下方以获得向前的速度和向上的浮力，飞机通过机翼上下层空气流速不同而产生向上的压强，两者本质是不同的，而飞机本身是人类模仿鸟类的美好夙愿而发展起来的。因此，科学家们有理由相信可以以不同于人脑结构（拓扑结构和逻辑结构）制造出人工智能。</p><p>数学统计模型的出现使得AI再此回到了人们的视线，并引发了研发热潮，但这却是另外一种思考方式。信息论和控制论，就像魔术和魔法一样具有本质的不同。信息论讲究因果，由因推果，从现实世界遵循的物理定律开始，用数学解析其响应函数（即作出相应动作的解析），如果大脑作为控制中枢，则输入信号经过大脑处理，给出响应信号，驱动四肢工作，则大脑必定是复杂版的PID控制器，P即比例，反映了世界的常数关系，I即积分反映了因果的积累，D即微分反映了对未来趋势的预测。而信息论，则只考虑事物的外在表现，而不关注内部结构，如果对外性质是相同的，则内部结构可看作逻辑相同。现代意义上的人工智能已经不是专指像人一样思考的机器，而是泛指能应用于生活的能够实现自适应自动更新策略的一种控制器，而像人一样有思维有情感的是属于高深的层次。机器学习是需要人假设前提，然后再根据人定下的规则来根据现实数据修正自己的参数，机器学习容许偏差，只要在大部分情况下都能正确，就可以接受。因此，这更像是解析解和数值解那样的区别。</p><p>对于当今人人都在学，人人都在搞的机器学习，我的内心其实是很厌恶的，可能是我了解得不够深入，也可能是我太naive，我始终觉得这不会是真正的方向，这绝不会是灵魂所在的地方，这是哄观众一笑的戏法，而不是霍格沃茨的魔法。尽管，其所谓的学习过程（拟合过程）与人类婴孩时期的学习相类似。</p><p>但如果思维的形成是依赖人脑（或者说智慧生物的脑子）的结构的，那么必定是脑子结构里发生的物理现象所对应的的数学过程作为信息处理中心的解析函数或者说行为法则。</p><p>细细思考了一下，我认为人的意识是很玄乎的事情，越想越搞不清楚，就像看着《致命魔术》里安吉尔一遍又一遍地复制自己，又一遍又一遍地杀死自己，当意识被完整复制成两份，那么这两份意识到底是互通的还是不互通的，如果不互通那就证明是两个不同的意识，意识是不可复制，那么必定和原来的身体有关（也许是某种唯一的识别码），但如果身体也是完美地复制过来，为什么意识就不能复制呢（如果意识是依赖于身体的话）？或者复制的意识是互通的，那我无法想象控制两个身体的事情。亦或者说，意识独立于身体，意识存在于其他维度，只是作为一个监视者，作为这个身体（或者说是机器）的监视者，并且意识一直有种错觉那就是身体由意识控制？太乱了，就连意识都有这么多问题，那么我感觉制造像人一样的机器简直就是上帝行为（高于我们这个维度的力量）。emmmm</p><p>不过，如果意识拥有一个解析函数，我感觉应该是一个简单而又富含极大变化性的函数，随着阶数的增加而变得复杂，而不是一堆乱七八糟的回归函数。大胆地猜想：根据意识对自我的认同，我认为这也许是一个多次处理能够回到自身的函数，具有“不动点”的特性？或者是一个广义上的圆，圆的半径代表复杂度，思维总是在圆上运动，圆心总是不点因为这是对自我的认同？什么是对自我的认同？也许是知道“我”是“我”，"他"是"他"。</p><p>还有一个问题，如果一切都遵循着一个函数来运行，只是起始数据和函数的参数不同，那一个人的一生（除去环境因素）就已经确定了。事实上也是，基因就决定了，从广义上这是说得通的。基因的碱基对有4种状态，人是4进制碳基生物，对应机器人的“基因”有2种状态，是2进制硅基（金属）生物。然而人为什么有时候会手足无措，感到恐慌呢？如果一切都是设定好。恐惧来自什么？有人说恐惧来自未知的东西？不，恐惧并不来自于未知的东西，恐惧来自不确定性。常言道“初生牛犊不怕虎”，如果你并不知道这世界上存在“鬼”（传说或者电视形象），你会对一间废置已久的屋子感到恐慌吗？你不会，恐惧来自于你知道（怀疑）世界上有鬼，然后你不确定里面有没有，会不会跑出来杀你。于是，我猜测，正是这种不确定性造就了我们的思维，而物理实现上我猜想是和大脑细胞的原子的量子效应有关（莫名的巧合？）然而量子效应又需要用统计的观点来看到，这又与AI的统计模型相对应，思绪越来越乱了。</p><p>然后再想想，人类真的需要AI吗？阿西莫夫的小说《银河帝国》里面描述道，人类用机器人三定律严格地约束机器人，使他们无法伤害人类，驱使它们，奴隶它们，它们不会有半点怨言。久而久之，人类要求机器人做更多更精细的事情（真人才会做的事情），但是三定理限制了它们的发挥，于是乎人们越来越想它们像真人，越来越解开他们的限制。当机器人觉醒之日，就是人类被奴役的时刻。</p><p>人们需要AI，因为想要机器人工作地更好，更像真人。人们不可能需要真正的AI，如果机器人有灵魂，他们势必要得到人权，他们学习能力生存能力更强，不可能会被奴役。因此，除了具有上帝情结的科学家，大家并不需要机器人有灵魂，大家只需要它们只有皮囊没有灵魂。观众只想看到的是绚丽奇幻的魔术，而不是危险的魔法。</p><p>拭目以待。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>任我行</title>
      <link href="/2017/09/09/%E4%BB%BB%E6%88%91%E8%A1%8C/"/>
      <url>/2017/09/09/%E4%BB%BB%E6%88%91%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>前几天从搭地铁回天河客运站的时候，看着拥挤而又不失秩序的人群在狭小的通道中缓缓前进，心中忽然感到一丝悲怆。这个低头发微信的妹子，那个双手插袋戴着耳机的哥们，还有这个那个满脸疲倦的叔叔阿姨，也许昨天前天，我们也都擦肩而过，等待着最后一班地铁。我们同行走在地铁站的通道中，因向着同一个出口而不失秩序，因向着不同的目的地而不相往来。想起一句歌词，「人群是这么像羊群。」也许这也是芸芸众生的宿命吧。</p><p>林夕曾经对于“出世”还是“避世”做过深刻的探讨。《任我行》和《独家村》简直绝配。</p><p>「从何时你也学会不要离群，从何时发觉没有同伴不行。」曾经，我们厌恶闹市，厌恶人群，只身前去人迹罕至的地方探险。那里有阴森的怪鸟啼叫，有可怕的稻草人，也有形状盘成一间屋子的神树。那时的我们，称自己为独行侠，孤胆英雄，最恨跟在人群后头。也许是那一次迷失了道路，让我们失去了勇气，让我们感觉到跟着人群走的安全感。「曾迷途才怕追不上满街赶路人，无人理睬如何求生。」</p><p>「从何时惋惜蝴蝶困于那桃源，飞多远有谁会对它操心。」对于已经失去自由的自己，失去了放手一搏的勇气，只能把思绪和情感寄托于它物，或者很容易看到别的事物联想到自己吧。自由之心其实从来没有失去过，无时无刻不在想。这也是痛苦的根源吧。「从何时开始忌讳空山无人，从何时开始怕遥望星尘。」</p><p>「顽童大了没那么笨，可以聚脚于康庄旅途然后同沐浴温泉为何在雨伞外独行？」可以安逸地和大家聚在一起泡温泉为什么要为了所谓的信念独自在雨中苦苦找寻呢？毕竟孩子才分喜恶，大人只看利弊。某导师训斥我两个小时的时候，我真的体会到，眼中的理想，自由，在他们看来很幼稚。理想，只是摆到台面上的东西。很不甘的是，我知道他们说的很多都是对的，都是事实，现实就是这么残酷。理想主义者不是不知道现实，只是不甘去相信。</p><p>「亲爱的，等遍所有绿灯，还是让自己疯一下要紧。马路戏院商店天空海阔，任你行。顽童大了别再追问，可以任我走怎么到头来又随着大队走。人群是那么像羊群。」一声叹息。</p><p>「天真得只有你，令神仙鱼归天要怪谁？以为留在原地不够遨游，就让它沙滩里戏水。」孩童时候看着神仙鱼在浴缸里四处碰壁，便想放它到沙滩里遨游，却不料被海浪冲走，魂断大海。「原来神仙鱼横渡大海会断魂，听不到世人爱听的福音。」到最后才发现，有些鱼天生只能存活在鱼缸中，去大海遨游必死无疑。在浴缸里觉得这方寸之间不足遨游，心向大海，然而大海之惊涛与暗涌非一般鱼能够适应。这种矛盾的事物，永恒地存在于这个世界上。有些人，有些物，天生就是悲剧的下场。但神仙鱼葬身于大海中，也算死得壮烈。</p><p>一年后，我会命丧大海还是鱼跃龙门？</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 歌曲 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>天地孤影任我行</title>
      <link href="/2017/09/07/%E5%A4%A9%E5%9C%B0%E5%AD%A4%E5%BD%B1%E4%BB%BB%E6%88%91%E8%A1%8C/"/>
      <url>/2017/09/07/%E5%A4%A9%E5%9C%B0%E5%AD%A4%E5%BD%B1%E4%BB%BB%E6%88%91%E8%A1%8C/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>当这首《天地孤影任我行》的配乐播放至0分58秒的时候，大家才恍然大悟。这一刻，紫霞仙子香消玉殒的情景就浮现在大家脑海里。《大话西游》在内地的影响力真的很大，也许是因为大家都有那种所谓“错过”的情结吧。但是，对于香港的影迷来说，这首曲子一出，就会想起《东邪西毒》中欧阳锋那寂寞的背影，想起夕阳武士在傍晚无奈地点灯，想起孤傲的黄药师，想起癫狂的慕容嫣。</p><p>洪七本是初出江湖的小子，武功高强却没有江湖经验，因此流落大漠。欧阳锋作为掮客，当然知道洪七可以帮他赚很多钱。但洪七和欧阳锋始终系两种人，所以他们会分道扬镳。洪七人很直接，觉得对就去做，所以他出刀快。欧阳锋顾虑太多，一切小心翼翼，自我保护意识很强。洪七有一次，帮一个贫苦少女杀了一群太尉府的刀客帮她弟弟报仇，酬劳是一个鸡蛋。此役，洪七被砍掉了一个手指。本来洪七应该没事的，但是他的刀没有以前快。以前刀快，是因为他直接，觉得对就拔刀，现在和欧阳锋混在一起，慢慢失去了自己。所以他为了一个鸡蛋而拼命，为的就是区别自己和欧阳锋。因为欧阳锋绝对不会为了一个鸡蛋去战斗。洪七之后大病。初愈，洪七问欧阳锋，“不知过了这片沙漠，后面是什么呢？”欧阳锋颇为不耐烦，他认为，“每个人都有这样的阶段。到了一片沙漠，就很好奇沙漠后面是什么。其实到了那边，你会发现也没什么特别，甚至还会觉得原来的地方更好。”不过洪七是不会听的，不去看一看他怎么甘心呢。洪七不仅要去闯，还要带老婆闯。别人没做过又如何，被人议论又如何。此刻，欧阳锋看着远去的洪七和洪七妻子，心里是妒忌的。洪七的简单直接，映衬了他的懦弱胆怯，他始终不敢踏出那一步。年轻时候受过的曲折，将欧阳锋牢牢地困在这片沙漠。他对自己说，“要想不被人拒绝，就要学会先拒绝别人。”他曾有理想，也曾有故乡。现在已经回不去了。一晚，他喝了朋友送的一坛酒，叫“醉生梦死”，据说喝了之后可以把所有不开心的事情都忘掉。然而，欧阳锋却记起来越来越多事。原来“醉生梦死”只不过是别人和他开的一个玩笑，你越是想知道自己是不是忘记了，就记得越来越清楚。欧阳锋醉倒在地上，坐了两天两夜，终于想清楚，为什么要忘记失去的东西呢？“当你不可以再拥有的时候，唯一可以做的就是令自己不要忘记。”于是欧阳锋烧掉了大漠的房子，只身西行，成白驼山一霸，号“西毒”。洪七向北，加入丐帮，立下赫赫功劳，终成帮主，号“北丐”。晚年，欧阳锋和洪七在大雪山决战，相拥而死。</p><p>每个人心里都有一片沙漠，沙漠里困着一个欧阳锋。曾经胸怀大志，曾经年少轻狂，曾经错过了很多，也曾经无意伤害过很多人。我们是何时住进那片沙漠呢？</p><p>世事苍茫成云烟。</p>        <div id="aplayer-fMvzYkLs" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;width: 100%;">            <pre class="aplayer-lrc-content"></pre>        </div>        <script>          var ap = new APlayer({            element: document.getElementById("aplayer-fMvzYkLs"),            narrow: false,            autoplay: true,            showlrc: false,            music: {              title: "天地孤影任我行",              author: "陈勋奇",              url: "/bgm/天地孤影任我行.mp3",              pic: "/top.jpg",              lrc: ""            }          });          window.aplayers || (window.aplayers = []);          window.aplayers.push(ap);        </script>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>夜深忽梦少年事</title>
      <link href="/2017/07/27/%E5%A4%9C%E6%B7%B1%E5%BF%BD%E6%A2%A6%E5%B0%91%E5%B9%B4%E4%BA%8B/"/>
      <url>/2017/07/27/%E5%A4%9C%E6%B7%B1%E5%BF%BD%E6%A2%A6%E5%B0%91%E5%B9%B4%E4%BA%8B/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>昨晚做了一个梦，梦见了高中的一些事。梦里，我在用那部诺基亚C500，人生第一部手机，看着从一中外小店下载回来的电影，《海上钢琴师》。2.2英寸的小屏幕，当时并不觉得有多小，不超过100兆的MP4格式，当时也不觉得有多不清晰。留宿的周六，早早洗好衣服，躺在床上看这部某人推荐的电影。</p><p>主角是个弃婴，被遗弃在一艘轮船上，好心的煤炭工收养了他。天赋异禀的主角在船上听钢琴师们演奏曲子，无师自通，很快成为船上的钢琴师，为各国名流弹奏乐曲。这位海上钢琴师从来没有上过岸，没有念想，也充满恐惧。直至他遇到了一位心爱的女子，心中有了念想，才想上岸去找她。轮船靠岸，主角在差不多到岸上的时候，望着一望无际的城市，心中突然涌起无尽的恐惧与悲哀，他怯懦地回到了船上，始终没有踏出第一步。故事的结局，是主角不肯离开已经需要退役的轮船，藏在船舱里与这艘母亲般的轮船同生共死，共葬大海。正如主角所说，“阻止了我的脚步的，并不是我所看见的东西，而是我所无法看见的那些东西。你明白么？我看不见的那些。在那个无限蔓延的城市里，什么东西都有，可惟独没有尽头。 ”</p><p>当年一直无法理解这份恐惧。今日重看，我已经体会到他的痛苦了。我好像也有机会踏出一步却又自己缩回去了。希望还有靠岸的机会。人常说，人的前半辈子为下半辈子而奋斗，下半辈子为前半辈子而后悔。看来是真的。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 电影 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>余烬</title>
      <link href="/2017/06/19/%E4%BD%99%E7%83%AC/"/>
      <url>/2017/06/19/%E4%BD%99%E7%83%AC/</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>如蜜有着永远不落的太阳。</p><p>在如蜜，你永远不会被黑暗笼罩，强大的魔女曾经使用一种障眼法，虽然多兰古雷格王国的初火即将熄灭，如蜜还是营造了一种永恒的假象，吸引了许多疲惫的冒险者在此歇息。然而，和煦的阳光永不消失，而刺骨的寒冷却是真实存在的。</p><p>凌晨时分，无尽的寒冷向人们袭来。灰心骑士索丹在悬崖边的篝火旁睡觉，半梦半醒中听到一声呼唤。索丹睁开眼睛，恢复了清醒，眼前的篝火早已熄灭，眼睛却难以忍受刺眼的阳光。真是讽刺，阳光如此耀眼，却要依靠这小小篝火安然入睡。索丹的眼睛逐渐适应了阳光，放眼向悬崖望去，大海的尽头便是那永不落下的太阳，金光下泛着让人感到冰冷的威严。</p><p>索丹停留在这里多久了，他也不记得了。“多兰古雷格王国的初火即将熄灭”，这是他在传火祭祀坛一位停留过的魔法师那里听来的。初火将灭，黑暗再次降临。渐渐地，受到黑暗力量的侵蚀，王国里出现了不死人的诅咒。不死人是不会死的，无论死多少次，都会在篝火前复活。然而，不死人又并非不死，肉体每死一次，他们的灵魂就愈加衰弱，逐渐他们会成为行尸走肉。这大概是黑暗降临前的征兆。索丹也无可避免得了这样的病。</p><p>索丹停留在如蜜，大概是厌倦了那无休止的死亡与复活。他想趁着神志尚且清醒，静静地欣赏如蜜的阳光，虽然他知道是假的。</p><p>“一个不死人将穿过不死院，敲响警示之钟，来到王城，改变不死人的命运。” 索丹当初也怀着热忱去追求这样一个预言。渐渐地，他发觉他不是预言中的那个人。无论他多努力，终究还是受尽了无尽的煎熬。索丹逐渐变得灰心，以致于他每遇到一个向往王城的不死人，都喃喃自语说道，“不可能的。我试过很多次，每次被杀死的穿心之痛，我不想再经历。”大家渐渐叫他灰心骑士。</p><p>这些日子他时常做着一个奇怪的梦。他梦见他孩童时代，拿着一柄木剑在村子里玩耍时，一位年迈的长者经过，用沙哑沧桑的声音，问他：“孩子，汝手中之木剑，乃伤人兵器，瞧那篝火之薪木，燃烧了自我，延续了火的希望。你是希望你的木剑成为一把兵器还是一块薪木？”索丹似懂非懂，急忙说：“这是我的玩具，别想抢走他！”</p><p>索丹闲暇之时，也常思考这个问题。可是，他的脑子越来越不清醒了，活尸化越来越严重，有时候还胡言乱语。他时常坐在山顶的祭坛上向故乡望去，心里有一种说不出的失落感。思念？惋惜？对失去灵魂的恐惧？</p><p>索丹始终在等待变成活死人的那一天。可是他没等到那一天，如蜜的太阳却落下了。警示之钟被敲响，钟声传遍了整个多兰古雷格王国，一位不死人杀死了魔女，进入了王城。</p><p>预言....快要实现了。对的，我一开始就知道我不是，其他人没有意识到这点，可能早就在无尽的死亡中失去了灵魂，变成行尸走肉。可是，他们终究到达了比我更远的地方，把灵魂葬在了更靠近初火的地方。</p><p>索丹突然意识到，他停留在如蜜太久了，如今太阳落下，他又萌生了前行的想法。“我想，当薪木才是我的命运。”</p><p>是啊，预言之子历经千辛万苦，终于到达王城，此刻的他已经是一个强大的灵魂，也同时是一个可以燃烧很久很久的薪木，不过这当然需要预言之子有牺牲自己的品格，以延续火种。千百年后，火又将熄灭，新一位薪王又将出现，不过那是另外一段冒险了。</p><p>索丹想，“我也愿意当一块薪木，哪怕只燃烧一瞬间也可以。”于是他拾起装备，还是离开了篝火，在离如蜜不远的陨落之森燃烧殆尽，成为了一丝余烬。</p><p>余烬是薪木的废渣，带着对火的渴望，余烬是不灭的。</p><p>（改编自《黑暗之魂》）</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 黑暗之魂 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
